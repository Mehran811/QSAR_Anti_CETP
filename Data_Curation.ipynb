{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26371726",
   "metadata": {},
   "source": [
    "# ChEMBL \n",
    "âœ… Final curated ChEMBL CETP dataset saved: 1393 compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a8d6a",
   "metadata": {},
   "source": [
    "âœ… Filtering biologically relevant IC50 range (1 fM â€“ 10 mM)\n",
    "\n",
    "âœ… Grouping by ligand-target system (step 2)\n",
    "\n",
    "âœ… Keeping the best pIC50 if multiple measurements exist (step 4)\n",
    "\n",
    "âœ… Detecting and removing rounded/duplicate/near-identical values (step 5)\n",
    "\n",
    "(ðŸ“Œ Steps 1 and 6 donâ€™t apply: youâ€™re using a valid target, and author data isn't available via the ChEMBL API.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed58dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chembl_webresource_client.new_client import new_client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Search for CETP target\n",
    "target_df = pd.DataFrame(new_client.target.search(\"CETP\"))\n",
    "target_id = target_df.loc[0, 'target_chembl_id']  # e.g., 'CHEMBL1824'\n",
    "\n",
    "# Step 2: Fetch all IC50 entries\n",
    "activities = []\n",
    "page = new_client.activity.filter(\n",
    "    target_chembl_id=target_id,\n",
    "    standard_type=\"IC50\"\n",
    ").only([\n",
    "    'molecule_chembl_id', 'canonical_smiles',\n",
    "    'standard_value', 'standard_units',\n",
    "    'standard_type', 'standard_relation', 'document_chembl_id'\n",
    "])\n",
    "data = list(page)\n",
    "activities.extend(data)\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "df = pd.DataFrame(activities)\n",
    "\n",
    "# Step 4: Filter rows with valid data\n",
    "df = df[\n",
    "    (df['standard_value'].notna()) &\n",
    "    (df['canonical_smiles'].notna())\n",
    "].copy()\n",
    "\n",
    "# Step 5: Rename and clean columns\n",
    "df = df.rename(columns={\n",
    "    'canonical_smiles': 'SMILES',\n",
    "    'standard_value': 'IC50_nM',\n",
    "    'document_chembl_id': 'DocumentID'\n",
    "})\n",
    "df['IC50_nM'] = pd.to_numeric(df['IC50_nM'], errors='coerce')\n",
    "df = df[(df['IC50_nM'] > 1e-6) & (df['IC50_nM'] < 1e7)]  # Kramer Step 3\n",
    "\n",
    "# Step 6: Convert to pIC50\n",
    "df['pIC50'] = -np.log10(df['IC50_nM'] * 1e-9)\n",
    "\n",
    "# Step 7: Create SystemID (ligand ID + SMILES)\n",
    "df['SystemID'] = df['molecule_chembl_id'] + \"_\" + df['SMILES']\n",
    "\n",
    "# Step 8: Keep best pIC50 per publication (Kramer Step 4)\n",
    "df = df.sort_values(['SystemID', 'DocumentID', 'pIC50'], ascending=[True, True, False])\n",
    "df = df.drop_duplicates(subset=['SystemID', 'DocumentID'], keep='first')\n",
    "\n",
    "# Step 9: Kramer Step 5 â€“ Remove rounded or near-duplicate pIC50 values across docs\n",
    "def flag_suspect(group):\n",
    "    vals = group['pIC50'].values\n",
    "    diffs = np.abs(np.subtract.outer(vals, vals))\n",
    "    if (np.any((diffs < 0.02) & (diffs > 0))) or np.any(np.isclose(vals % 3, 0, atol=0.01)) or np.any(np.isclose(vals % 6, 0, atol=0.01)):\n",
    "        return pd.Series([True] * len(group), index=group.index)\n",
    "    return pd.Series([False] * len(group), index=group.index)\n",
    "\n",
    "df['suspect_duplicate'] = df.groupby('SystemID', group_keys=False).apply(flag_suspect)\n",
    "df = df[~df['suspect_duplicate']]\n",
    "\n",
    "# Step 10: Final clean-up\n",
    "df = df[['molecule_chembl_id', 'SMILES', 'IC50_nM', 'pIC50', 'SystemID', 'DocumentID']]\n",
    "df = df.drop_duplicates(subset='SMILES')\n",
    "\n",
    "# Step 11: Save to file\n",
    "df.to_csv(\"chembl_cetp_ic50_curated.csv\", index=False)\n",
    "print(f\"âœ… Final curated ChEMBL CETP dataset saved: {len(df)} compounds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916306c",
   "metadata": {},
   "source": [
    "# BindingDB\n",
    "âœ… Final curated dataset saved: bindingdb_cetp_curated.csv (1286 compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7150d",
   "metadata": {},
   "source": [
    "New steps:  \n",
    "| Section                                    | Added Functionality         |\n",
    "| ------------------------------------------ | --------------------------- |\n",
    "| âœ… `SystemID` creation                      | Enables tracking duplicates |\n",
    "| âœ… pIC50 curation                           | Keeps best per publication  |\n",
    "| âœ… Rounding + transcription error filtering | Removes noisy duplications  |\n",
    "| âœ… Optional author filtering                | Removes biased entries      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Read headers and identify UniProt columns\n",
    "header = pd.read_csv(r\"C:\\QSAR\\QSAR_data\\BindingDB_All.tsv\", sep=\"\\t\", nrows=0)\n",
    "uniprot_cols = [col for col in header.columns if \"UniProt\" in col]\n",
    "\n",
    "# Step 2: Prepare for chunked reading\n",
    "chunksize = 10 ** 5\n",
    "filtered_chunks = []\n",
    "\n",
    "# Step 3: Process file in chunks\n",
    "for chunk in pd.read_csv(r\"C:\\QSAR\\QSAR_data\\BindingDB_All.tsv\", sep=\"\\t\", low_memory=False, chunksize=chunksize):\n",
    "    # Step 3.1: Filter rows with any UniProt column containing 'P11597'\n",
    "    mask = chunk[uniprot_cols].apply(lambda row: row.astype(str).str.contains('P11597', na=False)).any(axis=1)\n",
    "    filtered = chunk[mask]\n",
    "\n",
    "    # Step 3.2: Keep valid IC50 in nM range\n",
    "    filtered = filtered[filtered['IC50 (nM)'].notna()]\n",
    "    filtered['IC50_nM'] = pd.to_numeric(filtered['IC50 (nM)'], errors='coerce')\n",
    "    filtered = filtered[(filtered['IC50_nM'] > 1e-6) & (filtered['IC50_nM'] < 1e7)]  # between 1 fM and 10 mM\n",
    "\n",
    "    # Step 3.3: Handle SMILES column\n",
    "    smiles_col = 'Smiles' if 'Smiles' in filtered.columns else 'Ligand SMILES'\n",
    "    filtered = filtered[filtered[smiles_col].notna()]\n",
    "    filtered = filtered.rename(columns={smiles_col: 'SMILES'})\n",
    "\n",
    "    # Step 3.4: Create SystemID = SMILES + UniProt for grouping\n",
    "    filtered['UniProt_ID'] = filtered[uniprot_cols].bfill(axis=1).iloc[:, 0]  # use first non-null UniProt column\n",
    "    filtered['SystemID'] = filtered['SMILES'] + '_' + filtered['UniProt_ID']\n",
    "\n",
    "    # Step 3.5: Calculate pIC50\n",
    "    filtered['pIC50'] = -np.log10(filtered['IC50_nM'] * 1e-9)\n",
    "\n",
    "    # Step 3.6: Keep key columns for curation\n",
    "    keep_cols = ['BindingDB Reactant_set_id', 'SMILES', 'IC50_nM', 'pIC50', 'SystemID', 'PMID']\n",
    "    for col in keep_cols:\n",
    "        if col not in filtered.columns:\n",
    "            filtered[col] = np.nan\n",
    "    filtered = filtered[keep_cols]\n",
    "\n",
    "    filtered_chunks.append(filtered)\n",
    "\n",
    "# Step 4: Combine all chunks\n",
    "df = pd.concat(filtered_chunks, ignore_index=True)\n",
    "\n",
    "# Step 5: Kramer Step 4 â€” Keep highest pIC50 per SystemID + PMID\n",
    "df = df.sort_values(['SystemID', 'PMID', 'pIC50'], ascending=[True, True, False])\n",
    "df = df.drop_duplicates(subset=['SystemID', 'PMID'], keep='first')\n",
    "\n",
    "# Step 6: Kramer Step 5 â€” Remove suspect duplicates\n",
    "def flag_suspect(group):\n",
    "    vals = group['pIC50'].values\n",
    "    diffs = np.abs(np.subtract.outer(vals, vals))\n",
    "    if (np.any((diffs < 0.02) & (diffs > 0))) or np.any(np.isclose(vals % 3, 0, atol=0.01)) or np.any(np.isclose(vals % 6, 0, atol=0.01)):\n",
    "        return pd.Series([True] * len(group), index=group.index)\n",
    "    return pd.Series([False] * len(group), index=group.index)\n",
    "\n",
    "df['suspect_duplicate'] = df.groupby('SystemID', group_keys=False).apply(flag_suspect)\n",
    "df = df[~df['suspect_duplicate']]\n",
    "\n",
    "# Step 7: Kramer Step 6 â€” Drop by overlapping authors (optional)\n",
    "if 'Authors' in df.columns:\n",
    "    df['Authors'] = df['Authors'].fillna('')\n",
    "    df = df.sort_values(['SystemID', 'Authors'])\n",
    "    df = df.drop_duplicates(subset=['SystemID', 'Authors'], keep='first')\n",
    "\n",
    "# Step 8: Finalize and save\n",
    "df = df[['SystemID', 'SMILES', 'IC50_nM', 'pIC50', 'PMID']].drop_duplicates(subset='SMILES')\n",
    "df.to_csv(\"bindingdb_cetp_curated.csv\", index=False)\n",
    "print(f\"âœ… Final curated dataset saved: bindingdb_cetp_curated.csv ({len(df)} compounds)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697025ca",
   "metadata": {},
   "source": [
    "# Combining\n",
    "âœ… Final deduplicated dataset saved: combined_cetp_dataset.csv (1423 compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e92255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolToSmiles\n",
    "\n",
    "# Step 1: Load curated datasets\n",
    "df_binding = pd.read_csv(\"bindingdb_cetp_curated.csv\")\n",
    "df_chembl = pd.read_csv(\"chembl_cetp_ic50_curated.csv\")\n",
    "\n",
    "# Step 2: Keep only necessary columns and label dataset\n",
    "df_binding = df_binding[['SMILES', 'IC50_nM', 'pIC50']].copy()\n",
    "df_binding['dataset'] = 'BindingDB'\n",
    "\n",
    "df_chembl = df_chembl[['SMILES', 'IC50_nM', 'pIC50']].copy()\n",
    "df_chembl['dataset'] = 'ChEMBL'\n",
    "\n",
    "# Step 3: Combine both datasets\n",
    "df_combined = pd.concat([df_binding, df_chembl], ignore_index=True)\n",
    "\n",
    "# âœ… Step 4: Canonicalize SMILES using RDKit\n",
    "def canonicalize_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            return MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_combined['canonical_smiles'] = df_combined['SMILES'].apply(canonicalize_smiles)\n",
    "df_combined = df_combined.dropna(subset=['canonical_smiles'])\n",
    "\n",
    "# âœ… Step 5: Drop duplicates based on canonical SMILES\n",
    "df_combined = df_combined.drop_duplicates(subset='canonical_smiles')\n",
    "\n",
    "# âœ… Step 6: Label activity: IC50 < 50 nM â†’ Active\n",
    "df_combined['Activity'] = (df_combined['IC50_nM'] < 50).astype(int)\n",
    "\n",
    "# âœ… Step 7: Save final cleaned dataset\n",
    "df_combined = df_combined[['canonical_smiles', 'IC50_nM', 'pIC50', 'dataset', 'Activity']]\n",
    "df_combined.rename(columns={'canonical_smiles': 'SMILES'}, inplace=True)\n",
    "df_combined.to_csv(\"combined_chembel_bindingdb_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Final deduplicated dataset saved: combined_cetp_dataset.csv ({len(df_combined)} compounds)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
